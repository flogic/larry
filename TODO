
 - there are pools of hosts, so that we can target deployments to different pools
 - we can provision hosts that don't exist
 - we can provision hosts into a pool
 - we can ask a pool to provision hosts
 - hosts can have capacities, so that we can estimate whether we have enough capacity for deploying instances
 - instances can have costs, so that we can determine if we have sufficient host resources to deploy instances
 - should be able to determine if a pool has sufficient capacity to deploy instance(s)
 - should be able to recommend hosts to provision
 - [ provision would be via rubycloud, etc. ]

 - when deploying an instance we specify a pool, so that capacity measurement and provisioning can work (also so shit gets on the right machines, duh)
 - we can deploy instances multiple times, even concurrently, so that we can do testing of instances, etc.
 - when we deploy an instance we create a record of that deployment with sufficient information to deploy to a pool, so that we can rollback a deployment, test a deployment in different places, deploy concurrently, etc.
 - when we deploy an instance we create a log of the deployment, which notes which actual hosts we deployed which services to, so that we can actually get the instances running on real systems
 - we can create a start/end time window for deployments, so that we can schedule future deployments, terminate deployments, and see which deployments are actually current
 - we can generate a manifest for a host by looking at all the deployed services for deployments which are within the current time window
 - we can mark various things (instance, app, customer, host, ?) as "dirty" on changes, and save them up for a deployment, so that we can know if our planned changes have been put into action, know when we need to make a deployment, can batch up changes, and keep the process of creating deployments separate from the data which is used to do current production deployments
 - since deployment records are captured we can do rollbacks
 - since deployment records are captured we can run a tool like puppet consistently over them without fear of introducing changes
 - since deployment manifests can only change when a deployment is created or on the time-interval boundaries of the deployments, we don't have to continuously poll for changes (i.e., we could schedule and/or notify instead).

 

---

 - deployments need a deployment instance -- that is, a time period when a deployment is valid.
   - deployment instance would have a description, a start time and end time, a state (not deployed, deploying, deployed)
   - ideally we're trying to create a record of what a deployment state is, schedule future deployments, be able to check a deployment to see if it will work, run a new deployment up before taking an old one down, etc.
   - there is complexity, though, in that if the service graph changes, e.g., then current deployments are out of date, and things would need to be "re-deployed" to make things happy; this sounds like a lot of time-varying overhead on services, so this is probably overkill; similarly if parameter values for customer, app, instance change (or parameter lists for services), then, the current deployments are out of date, the instance is "dirty" and needs to be redeployed; what does a deployment snapshot look like?  it's instance + parameter settings, + service list, + maybe something about what types of hosts it can go to (which isn't even implemented anyway, but maybe that's the just a destinations snapshot).  So, in theory a Deployment (which is the deployment instance) is an object with a snapshot, and we can have DeploymentEvent records which are when a Deployment happens; all of which corresponds to instantiating some DeployedService instances (maybe the new name for the small thing currently called deployment)
 
 - deployments
 - deployment periods,
 - audits
 - make sure this thing can actually trigger a reasonable build (say, via the puppet variable/class scoping wedge in doc/vendor)

--- Questions

 Q: are there parameters for services that would be accepted but are not required (i.e., they should auto-fill in our instance parameter list, but not display as "required", and a can_deploy? call would ignore them)?
  
 Q: do we need to check can_deploy? when generating a manifest from Instance?

 Q: is the difference between a service that's required by many instances on a host (e.g., postgres) but which only really causes one installation to happen, vs. services that have the same name but differ in parameters and cause multiple things to happen (e.g., setting up a postgres *database* for an app) ... is this something larry needs to concern itself with, or do we just leverage the underlying management tool?  If we're using puppet, puppet can sort things out, but it's not clear that something like chef, e.g., can.

--- Low Priority Crap
 
 -> deploy.  This may imply that we have a state machine associated with deployments (associated with instances/services).  This would also imply that getting deployment feedback (e.g., the puppet-reporter "report" data closing the loop, if that's possible; or monitoring information being able to be fed back in; both are tricky but at least one of them would be highly useful, imo)

 -> would be useful for customer, app, instance to have deployed? predicates so that we can know if they are deployed anywhere

 *** -> brainstorming:  we could aggregate a set of Deployments by a Time Period, which reflects an interval in time when this deployment was true, is & will be true, or will be true in the future.  We could also archive off a deployment set (even if we do nothing else with it) to generate a deployment history.  This could make reversion to a prior state possible.
---

 -> start implementing Destination stuff (see below) so we can make Instance#can_deploy? more meaningful

 "Audits":  aka, Service linkages that use Destinations to compute which Hosts they are to be deployed to (e.g., monitoring, backups, firewall rules, logrotate, etc.).  Services *about* other services.
  
 ... Q:  is there any reason not to use Destinations for *all* deployments?  Meaning that audits are only distinguished then by their deployments' explicit association (aboutness) with another deployment.

 - Currently we model the instance -> deployment -> host relationship in such a way that we presume every service for an instance is deployed to the same host; audits make this untrue, and hence the cardinality of the instance -> host(s) relationship also has to increase, as well as the cardinality of instance -> deployment ... need to revisit the domain_model graphic
 
 - so, this is where host classification might actually be useful.  Make various host classes ("database server", "web server", "high throughput box", etc.), then have Destinations for Services be by host class membership.
 
 - note also that host class membership may set a floor on the type of provisioning we can do (or, at least, could say, this host class requires something of this size or larger, cf, the rubycloud notion of per-provider flavors, and so any of the flavors mapped to this size or larger could suffice)

 ---
 
 -> operations:  un-deploy, deploy, shut down, start host, provision new host, etc. (what impact do these have out in the world?)

 -> we should enforce that only top-level services are linked to instances, everything else is computed ... this means that when edges are created, or when services are linked/unlinked from instances that there may be a cascade of changes to the service set for an instance, which may cascade to deployments, which may cascade out to various hosts -- this would work best if we live in a do/undo world of host deployment (e.g., the way stahnma is setting up puppet manifests)


 -> could have indexes highligh strange situations:  customers with undeployed apps, hosts with no deployments, apps which are not deployed, etc.

 ??? there should be a services method for hosts, so that we can easily see what services are supposed to be on the host.  The deployments are, in a sense, the detailed "Actually deployed on the host" services.  So, in a sense, the services are being linked via destinations to deployments on hosts (if the destination is a same-host destination then destination is almost a no-op way to get from service to deployment, which belongs_to host)

 -> generate some sort of YAML dump
 -> import data from YAML dump
 -> probably useful to have a display of services that are not deployed for instances (why did we not deploy the searchd for this site?)
 -> favicon.ico, heh
 
 ---

 -> add linkages to nagios details for a host, service, etc.
 -> add linkages from customers to invoicing, e.g.
 -> is it possible to link from apps (or instances) to github pages?
 -> is it possible to link apps / customer to trouble ticket pages?
 -> what about linking to PT, e.g.
 
 : this probably speaks to a general facility to associate an URL with a class (customer, app, instance, deployment, host, etc.); in which case it might well be possible to have parsers for the data at the end of those URLs so that we could aggregate data; if we push out RSS, e.g., from larry then it would be possible for larry to be a central dashboard.

 ... so, if there is a notion of a Deployment event, then it would be possible to say that at the time of that Deployment (make a timestamp), the various services represented the current state of, say, puppet manifest files.  It could conceivably be possible to get a git revision # associated with those manifests from a well-known repo.  Other than file contents (which could, with architecture changes, be git revision #'d) this should be sufficient to deploy the configuration to a host *as it was then*.  If the instance filesystem images (e.g., for a slice) are similarly revision #'d then it might be possible to actually provision a node, as it was then, deploy the OS and software, as it was then, push the services out, with file contents, as they were then, etc.  Which would meet the conditions for being able to do a rollback, regardless of the time since the deployment.  A pipe dream, but oh well.  

